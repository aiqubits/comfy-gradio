# gradio
torch --extra-index-url https://download.pytorch.org/whl/cu126
gradio>=5.0.0
accelerate>=0.26.0
socksio
qwen_vl_utils
loguru
dashscope
packaging
setuptools
wheel
flash_attn   
# pip install --timeout=10000 "git+https://github.com/Dao-AILab/flash-attention.git"
# pip install "flash-attention@git+https://github.com/Dao-AILab/flash-attention.git"
# pip install flash-attn --no-build-isolation
