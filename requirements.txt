# gradio
gradio>=5.0.0
#accelerate>=0.26.0
socksio
#qwen_vl_utils
loguru
dashscope
#flash_attn   
# pip install --timeout=10000 "git+https://github.com/Dao-AILab/flash-attention.git"
# pip install "flash-attention@git+https://github.com/Dao-AILab/flash-attention.git"
# pip install flash-attn --no-build-isolation
